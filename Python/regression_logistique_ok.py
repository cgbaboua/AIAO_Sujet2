# -*- coding: utf-8 -*-
"""Regression_Logistique_OK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TKI0Q2SpnEYrPvsE_sDfFfLnkcr34zt0
"""

# Ouverture du data

"""# Nouvelle section"""

!pip install fastparquet

import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import os
import requests
from tqdm.auto import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve,roc_auc_score
import seaborn as sns
import fastparquet
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

train_data = pd.read_parquet('/content/drive/MyDrive/AIAO_Kaggle/Data/train_series_preprocessed_moi.parquet')
train_data

# Sélection des variables indépendantes
X = train_data[['anglez', 'enmo']]
X

# Variable dépendante (binaire)
Y = train_data['event']

Y_binary = np.zeros(len(Y))
Y_binary[Y == 'wakeup'] = 1
Y = Y_binary
print(Y)
len(Y)

"""### SECTION PROVISOIRE ########"""

# On prend que 6000 lignes au hasard dans X (pour faire un essai sur un petit échantillon)

indices_aleatoires = np.random.choice(X.index, size=6000, replace=False)
X = X.loc[indices_aleatoires]
Y = Y[indices_aleatoires]

Y

"""### SECTION PRINCIPALE ###

Mise à l'échelle des données
"""

# Mise à l'échelle
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Affichage avec boxplot
plt.boxplot(X, labels=['anglez', 'enmo'])
plt.title("Données mises à l'échelle")
plt.xlabel("Variables")
plt.ylabel("Valeurs mises à l'échelle")
plt.show()



X

"""Analyse descriptives par groupe"""

# Moyenne, variance, et écart-type
#grouped = train_data.groupby('event').agg(['mean', 'var', 'std'])

#print(grouped)

sample_data = train_data.sample(frac=0.01)  # Sur 1% des données pour exemple
grouped_sample = sample_data.groupby('event').agg(['mean', 'var', 'std'])
print(grouped_sample)

"""Représentation graphique"""

# Boxplot
sns.boxplot(x="event", y="anglez", data=train_data)

# Division des données en ensemble d'entraînement et de test
unique_series_ids = train_data['series_id'].unique()
train_series_ids, test_series_ids = train_test_split(unique_series_ids, test_size=0.2, random_state=42)

train_data['set'] = train_data['series_id'].apply(lambda x: 'train' if x in train_series_ids else 'test')
train_data_filtered = train_data[train_data['set'] == 'train']
test_data_filtered = train_data[train_data['set'] == 'test']

X_train = train_data_filtered[['anglez', 'enmo']].to_numpy()
y_train = train_data_filtered['event'].apply(lambda x: 1 if x == 'wakeup' else 0).to_numpy()

X_test = test_data_filtered[['anglez', 'enmo']].to_numpy()
y_test = test_data_filtered['event'].apply(lambda x: 1 if x == 'wakeup' else 0).to_numpy()

del train_data['set']

X_train
len(X_train)

X_test
len(X_test)

y_train
len(y_train)

y_test
len(y_test)

"""Corrélation"""

selected_columns = [col for col in train_data.columns if col != 'step']

# Calcul de la matrice de corrélation
#cor_matrix = train_data.corr()
cor_matrix = train_data[selected_columns].corr()
# Affichage
sns.heatmap(cor_matrix, annot=True)

# Recherche de corrélations élevées
high_correlation = cor_matrix[cor_matrix > 0.9]
print(high_correlation)

"""Modèle de regression logistique"""

log_reg = LogisticRegression(random_state=0)
log_reg.fit(X_train, y_train)

# Evaluation du modèle sur l'ensemble d'entraînement
y_train_pred = log_reg.predict(X_train)
print(classification_report(y_train, y_train_pred))

ax = sns.heatmap(confusion_matrix(y_train, y_train_pred, normalize="true"), annot=True)
ax.set(xlabel="y_pred", ylabel="y_true")
plt.show()

# Evaluation du modèle sur l'ensemble de test
y_test_pred = log_reg.predict(X_test)
print(classification_report(y_test, y_test_pred))

ax = sns.heatmap(confusion_matrix(y_test, y_test_pred, normalize="true"), annot=True)
ax.set(xlabel="y_pred", ylabel="y_true")
plt.show()

log_reg = LogisticRegression()
log_reg.fit(X,Y)

# Scores de la classe positive (ici 'wakeup' qui a été codé comme 1)
y_test_scores = log_reg.predict_proba(X_test)[:, 1]

# Convertir X_test à un DataFrame si ce n'est pas déjà le cas (au cas où X_test est un array numpy)
if isinstance(X_test, np.ndarray):
    X_test_1 = pd.DataFrame(X_test, columns=['anglez', 'enmo'])

# Ajouter les nouvelles colonnes au DataFrame
X_test_1['y_pred'] = y_test_pred

# Montrer le nouveau DataFrame
X_test_1

# Convertir les prédictions en événements lisibles (par exemple, "wakeup" pour 1)
y_test_pred_events = np.where(y_test_pred==1, 'wakeup', 'onset')  # Assumer que 1 correspond à 'wakeup'

# Convertir y_test_pred en une série pandas avec les mêmes indices que X_test
y_test_pred_series = pd.Series(y_test_pred_events, index=X_test_1.index)

# Extraire les lignes correspondantes de train_data en utilisant les indices de X_test
subset_train_data = train_data.loc[X_test_1.index]

# Ajouter les prédictions en tant que nouvelle colonne 'event'
subset_train_data['event'] = y_test_pred_series
# A commenter si on veut pas les scores
#subset_train_data['y_scores'] = y_test_scores
# Montrer le DataFrame résultant
subset_train_data_sorted = subset_train_data.sort_values(by=['series_id', 'timestamp']).reset_index(drop=True)

subset_train_data_sorted

subset_train_data_sorted[subset_train_data_sorted['event'] == 'onset']

#Verification de la cohérence
subset_train_data_sorted[subset_train_data_sorted['series_id']=='10f8bc1f7b07']

train_data[train_data['series_id']=='10f8bc1f7b07']

"""## SECTION PEUT-ÊTRE INUTILE"""

y_pred = log_reg.predict(X)
cm = confusion_matrix(Y, y_pred)
acc = accuracy_score(Y, y_pred)
auc = roc_auc_score(Y, y_pred)

print(y_pred)
print(cm)
print(acc)
print(auc)

y_prob = log_reg.predict_proba(X)[:,1]
fpr, tpr, thresholds = roc_curve(Y, y_prob)

plt.plot(fpr, tpr,label=f'AUC = {auc:.2f}')
plt.xlabel('Taux de faux positifs')
plt.ylabel('Taux de vrais positifs')
plt.legend(loc='lower right')
plt.show()

X_test, y_test

"""## TEST CALCUL SCORE"""

tolerances = {
    "wakeup": [1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30],
    "onset": [1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30]
}

# Renommer les colonnes pour correspondre au format de soumission
submission_data = subset_train_data_sorted.rename(columns={
    'y_scores': 'score'
})

# Réorganiser les colonnes et ajouter row_id
submission_data = submission_data[['series_id', 'step', 'event', 'score']]
submission_data['row_id'] = range(len(submission_data))

# Réordonner les colonnes
submission_data = submission_data[['row_id', 'series_id', 'step', 'event', 'score']]

# Sauvegarder le DataFrame dans un fichier CSV pour la soumission
submission_data.to_csv('submission.csv', index=False)

submission_data

solution_data = train_data.loc[X_test_1.index]

solution_data = solution_data.sort_values(by=['series_id', 'timestamp']).reset_index(drop=True)

solution_data_ok = solution_data[['series_id', 'timestamp', 'event']]
solution_data_ok

# Conversion des timestamps en secondes depuis l'époque Unix
solution_data_ok['timestamp'] = solution_data_ok['timestamp'].apply(lambda x: pd.Timestamp(x).timestamp())

# Trouver le timestamp minimum et soustraire de tous les autres
start_time = solution_data_ok['timestamp'].min()
solution_data_ok['timestamp'] = solution_data_ok['timestamp'] - start_time

solution_data_ok

column_names = {
    'series_id_column_name': 'series_id',
    'time_column_name': 'timestamp',
    'event_column_name': 'event',
    'score_column_name': 'score',
}

score(solution_data_ok,submission_data,tolerances,**column_names)

#Si on veut comparer

solution_data_to_compare = solution_data.copy()
solution_data_to_compare['event_pred']= subset_train_data_sorted['event']
solution_data_to_compare
#solution_data_to_compare[solution_data_to_compare['series_id']=='10f8bc1f7b07']

"""## TEST SUR LE DATA_TEST


"""

test_data = pd.read_parquet('/content/drive/MyDrive/Projet_Kaggle/PRE-FINAL_DATA/test_series.parquet')
test_data

X_true_test = test_data[['anglez', 'enmo']]
X_true_test = scaler.fit_transform(X_true_test)
X_true_test

# Evaluation du modèle sur l'ensemble de test
y_pred = log_reg.predict(X_true_test)
y_pred_scores = log_reg.predict_proba(X_true_test)[:, 1]

# Création d'un nouveau DataFrame pour stocker les résultats
test_data_result = test_data.copy()
test_data_result['y_pred'] = y_pred
test_data_result['score'] = y_pred_scores


test_data_result['y_pred'] = test_data_result['y_pred'].replace(0, "onset")
test_data_result['y_pred'] = test_data_result['y_pred'].replace(1, "wakeup")

test_data_result

test_data_result[test_data_result['y_pred'] == 'onset']

"""# CALCUL DU SCORE - TEST"""

"""Event Detection Average Precision

An average precision metric for event detection in time series and
video.

"""

import numpy as np
import pandas as pd
import pandas.api.types
from typing import Dict, List, Tuple


class ParticipantVisibleError(Exception):
    pass


# Set some placeholders for global parameters
series_id_column_name = None
time_column_name = None
event_column_name = None
score_column_name = None
use_scoring_intervals = None


def score(
        solution: pd.DataFrame,
        submission: pd.DataFrame,
        tolerances: Dict[str, List[float]],
        series_id_column_name: str,
        time_column_name: str,
        event_column_name: str,
        score_column_name: str,
        use_scoring_intervals: bool = False,
) -> float:
    """Event Detection Average Precision, an AUCPR metric for event detection in
    time series and video.

    This metric is similar to IOU-threshold average precision metrics commonly
    used in object detection. For events occuring in time series, we replace the
    IOU threshold with a time tolerance.

    Submissions are evaluated on the average precision of detected events,
    averaged over timestamp error tolerance thresholds, averaged over event
    classes.

    Detections are matched to ground-truth events within error tolerances, with
    ambiguities resolved in order of decreasing confidence.

    Detailed Description
    --------------------
    Evaluation proceeds in four steps:

    1. Selection - (optional) Predictions not within a series' scoring
    intervals are dropped.
    2. Assignment - Predicted events are matched with ground-truth events.
    3. Scoring - Each group of predictions is scored against its corresponding
    group of ground-truth events via Average Precision.
    4. Reduction - The multiple AP scores are averaged to produce a single
    overall score.

    Selection

    With each series there may be a defined set of scoring intervals giving the
    intervals of time over which zero or more ground-truth events might be
    annotated in that series. A prediction will be evaluated only if it falls
    within a scoring interval. These scoring intervals can be chosen to improve
    the fairness of evaluation by, for instance, ignoring edge-cases or
    ambiguous events.

    It is recommended that, if used, scoring intervals be provided for training
    data but not test data.

    Assignment

    For each set of predictions and ground-truths within the same `event x
    tolerance x series_id` group, we match each ground-truth to the
    highest-confidence unmatched prediction occurring within the allowed
    tolerance.

    Some ground-truths may not be matched to a prediction and some predictions
    may not be matched to a ground-truth. They will still be accounted for in
    the scoring, however.

    Scoring

    Collecting the events within each `series_id`, we compute an Average
    Precision score for each `event x tolerance` group. The average precision
    score is the area under the (step-wise) precision-recall curve generated by
    decreasing confidence score thresholds over the predictions. In this
    calculation, matched predictions over the threshold are scored as TP and
    unmatched predictions as FP. Unmatched ground-truths are scored as FN.

    Reduction

    The final score is the average of the above AP scores, first averaged over
    tolerance, then over event.

    Parameters
    ----------
    solution : pd.DataFrame, with columns:

        `series_id_column_name` identifier for each time series

        `time_column_name` the time of occurence for each event as a numeric type

        `event_column_name` class label for each event

        The solution contains the time of occurence of one or more types of
        event within one or more time series. The metric expects the solution to
        contain the same event types as those given in `tolerances`.

        When `use_scoring_intervals == True`, you may include `start` and `end`
        events to delimit intervals within which detections will be scored.
        Detected events (from the user submission) outside of these events will
        be ignored.

    submission : pd.DataFrame, with columns as above and in addition:

        `score_column_name` the predicted confidence score for the detected event

    tolerances : Dict[str, List[float]]

        Maps each event class to a list of timestamp tolerances used
        for matching detections to ground-truth events.

    use_scoring_intervals: bool, default False

        Whether to ignore predicted events outside intervals delimited
        by `'start'` and `'end'` events in the solution. When `False`,
        the solution should not include `'start'` and `'end'` events.
        See the examples for illustration.

    Returns
    -------
    event_detection_ap : float
        The mean average precision of the detected events.

    Examples
    --------
    Detecting `'pass'` events in football:
    >>> column_names = {
    ...     'series_id_column_name': 'video_id',
    ...     'time_column_name': 'time',
    ...     'event_column_name': 'event',
    ...     'score_column_name': 'score',
    ... }
    >>> tolerances = {'pass': [1.0]}
    >>> solution = pd.DataFrame({
    ...     'video_id': ['a', 'a'],
    ...     'event': ['pass', 'pass'],
    ...     'time': [0, 15],
    ... })
    >>> submission = pd.DataFrame({
    ...     'video_id': ['a', 'a', 'a'],
    ...     'event': ['pass', 'pass', 'pass'],
    ...     'score': [1.0, 0.5, 1.0],
    ...     'time': [0, 10, 14.5],
    ... })
    >>> score(solution, submission, tolerances, **column_names)
    1.0

    Increasing the confidence score of the false detection above the true
    detections decreases the AP.
    >>> submission.loc[1, 'score'] = 1.5
    >>> score(solution, submission, tolerances, **column_names)
    0.6666666666666666...

    Likewise, decreasing the confidence score of a true detection below the
    false detection also decreases the AP.
    >>> submission.loc[1, 'score'] = 0.5  # reset
    >>> submission.loc[0, 'score'] = 0.0
    >>> score(solution, submission, tolerances, **column_names)
    0.8333333333333333...

    We average AP scores over tolerances. Previously, the detection at 14.5
    would match, but adding smaller tolerances gives AP scores where it does
    not match. This results in both a FN, since the ground-truth wasn't
    detected, and a FP, since the detected event matches no ground-truth.
    >>> tolerances = {'pass': [0.1, 0.2, 1.0]}
    >>> score(solution, submission, tolerances, **column_names)
    0.3888888888888888...

    We also average over time series and over event classes.
    >>> tolerances = {'pass': [0.5, 1.0], 'challenge': [0.25, 0.50]}
    >>> solution = pd.DataFrame({
    ...     'video_id': ['a', 'a', 'b'],
    ...     'event': ['pass', 'challenge', 'pass'],
    ...     'time': [0, 15, 0],  # restart time for new time series b
    ... })
    >>> submission = pd.DataFrame({
    ...     'video_id': ['a', 'a', 'b'],
    ...     'event': ['pass', 'challenge', 'pass'],
    ...     'score': [1.0, 0.5, 1.0],
    ...     'time': [0, 15, 0],
    ... })
    >>> score(solution, submission, tolerances, **column_names)
    1.0

    By adding scoring intervals to the solution, we may choose to ignore
    detections outside of those intervals.
    >>> tolerances = {'pass': [1.0]}
    >>> solution = pd.DataFrame({
    ...     'video_id': ['a', 'a', 'a', 'a'],
    ...     'event': ['start', 'pass', 'pass', 'end'],
    ...     'time': [0, 10, 20, 30],
    ... })
    >>> submission = pd.DataFrame({
    ...     'video_id': ['a', 'a', 'a'],
    ...     'event': ['pass', 'pass', 'pass'],
    ...     'score': [1.0, 1.0, 1.0],
    ...     'time': [10, 20, 40],
    ... })
    >>> score(solution, submission, tolerances, **column_names, use_scoring_intervals=True)
    1.0

    """
    # Validate metric parameters
    assert len(tolerances) > 0, "Events must have defined tolerances."
    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\
        (f"Solution column {event_column_name} must contain the same events "
         "as defined in tolerances.")
    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\
        f"Solution column {time_column_name} must be of numeric type."

    # Validate submission format
    for column_name in [
        series_id_column_name,
        time_column_name,
        event_column_name,
        score_column_name,
    ]:
        if column_name not in submission.columns:
            raise ParticipantVisibleError(f"Submission must have column '{target_name}'.")

    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):
        raise ParticipantVisibleError(
            f"Submission column '{time_column_name}' must be of numeric type."
        )
    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):
        raise ParticipantVisibleError(
            f"Submission column '{score_column_name}' must be of numeric type."
        )

    # Set these globally to avoid passing around a bunch of arguments
    globals()['series_id_column_name'] = series_id_column_name
    globals()['time_column_name'] = time_column_name
    globals()['event_column_name'] = event_column_name
    globals()['score_column_name'] = score_column_name
    globals()['use_scoring_intervals'] = use_scoring_intervals

    return event_detection_ap(solution, submission, tolerances)


def filter_detections(
        detections: pd.DataFrame, intervals: pd.DataFrame
) -> pd.DataFrame:
    """Drop detections not inside a scoring interval."""
    detection_time = detections.loc[:, time_column_name].sort_values().to_numpy()
    intervals = intervals.to_numpy()
    is_scored = np.full_like(detection_time, False, dtype=bool)

    i, j = 0, 0
    while i < len(detection_time) and j < len(intervals):
        time = detection_time[i]
        int_ = intervals[j]

        # If the detection is prior in time to the interval, go to the next detection.
        if time < int_.left:
            i += 1
        # If the detection is inside the interval, keep it and go to the next detection.
        elif time in int_:
            is_scored[i] = True
            i += 1
        # If the detection is later in time, go to the next interval.
        else:
            j += 1

    return detections.loc[is_scored].reset_index(drop=True)


def match_detections(
        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame
) -> pd.DataFrame:
    """Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group."""
    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()
    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)
    gts_matched = set()
    for i, det in enumerate(detections_sorted.itertuples(index=False)):
        best_error = tolerance
        best_gt = None

        for gt in ground_truths.itertuples(index=False):
            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))
            if error < best_error and gt not in gts_matched:
                best_gt = gt
                best_error = error

        if best_gt is not None:
            is_matched[i] = True
            gts_matched.add(best_gt)

    detections_sorted['matched'] = is_matched

    return detections_sorted


def precision_recall_curve(
        matches: np.ndarray, scores: np.ndarray, p: int
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    if len(matches) == 0:
        return [1], [0], []

    # Sort matches by decreasing confidence
    idxs = np.argsort(scores, kind='stable')[::-1]
    scores = scores[idxs]
    matches = matches[idxs]

    distinct_value_indices = np.where(np.diff(scores))[0]
    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]
    thresholds = scores[threshold_idxs]

    # Matches become TPs and non-matches FPs as confidence threshold decreases
    tps = np.cumsum(matches)[threshold_idxs]
    fps = np.cumsum(~matches)[threshold_idxs]

    precision = tps / (tps + fps)
    precision[np.isnan(precision)] = 0
    recall = tps / p  # total number of ground truths might be different than total number of matches

    # Stop when full recall attained and reverse the outputs so recall is non-increasing.
    last_ind = tps.searchsorted(tps[-1])
    sl = slice(last_ind, None, -1)

    # Final precision is 1 and final recall is 0
    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]


def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:
    precision, recall, _ = precision_recall_curve(matches, scores, p)
    # Compute step integral
    return -np.sum(np.diff(recall) * np.array(precision)[:-1])


def event_detection_ap(
        solution: pd.DataFrame,
        submission: pd.DataFrame,
        tolerances: Dict[str, List[float]],
) -> float:

    # Ensure solution and submission are sorted properly
    solution = solution.sort_values([series_id_column_name, time_column_name])
    submission = submission.sort_values([series_id_column_name, time_column_name])

    # Extract scoring intervals.
    if use_scoring_intervals:
        intervals = (
            solution
            .query("event in ['start', 'end']")
            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())
            .pivot(
                index='interval',
                columns=[series_id_column_name, event_column_name],
                values=time_column_name,
            )
            .stack(series_id_column_name)
            .swaplevel()
            .sort_index()
            .loc[:, ['start', 'end']]
            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)
        )

    # Extract ground-truth events.
    ground_truths = (
        solution
        .query("event not in ['start', 'end']")
        .reset_index(drop=True)
    )

    # Map each event class to its prevalence (needed for recall calculation)
    class_counts = ground_truths.value_counts(event_column_name).to_dict()

    # Create table for detections with a column indicating a match to a ground-truth event
    detections = submission.assign(matched = False)

    # Remove detections outside of scoring intervals
    if use_scoring_intervals:
        detections_filtered = []
        for (det_group, dets), (int_group, ints) in zip(
            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)
        ):
            assert det_group == int_group
            detections_filtered.append(filter_detections(dets, ints))
        detections_filtered = pd.concat(detections_filtered, ignore_index=True)
    else:
        detections_filtered = detections

    # Create table of event-class x tolerance x series_id values
    aggregation_keys = pd.DataFrame(
        [(ev, tol, vid)
         for ev in tolerances.keys()
         for tol in tolerances[ev]
         for vid in ground_truths[series_id_column_name].unique()],
        columns=[event_column_name, 'tolerance', series_id_column_name],
    )

    # Create match evaluation groups: event-class x tolerance x series_id
    detections_grouped = (
        aggregation_keys
        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')
        .groupby([event_column_name, 'tolerance', series_id_column_name])
    )
    ground_truths_grouped = (
        aggregation_keys
        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')
        .groupby([event_column_name, 'tolerance', series_id_column_name])
    )
    # Match detections to ground truth events by evaluation group
    detections_matched = []
    for key in aggregation_keys.itertuples(index=False):
        dets = detections_grouped.get_group(key)
        gts = ground_truths_grouped.get_group(key)
        detections_matched.append(
            match_detections(dets['tolerance'].iloc[0], gts, dets)
        )
    detections_matched = pd.concat(detections_matched)

    # Compute AP per event x tolerance group
    event_classes = ground_truths[event_column_name].unique()
    ap_table = (
        detections_matched
        .query("event in @event_classes")
        .groupby([event_column_name, 'tolerance']).apply(
            lambda group: average_precision_score(
                group['matched'].to_numpy(),
                group[score_column_name].to_numpy(),
                class_counts[group[event_column_name].iat[0]],
            )
        )
    )
    # Average over tolerances, then over event classes
    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)

    return mean_ap

tolerances = {
    "Awake": [1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30],
    "Onset": [1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30]
}

# Renommer les colonnes pour correspondre au format de soumission
submission_data = test_data_result.rename(columns={
    'y_pred': 'event',
    'score': 'score'
})

# Réorganiser les colonnes et ajouter row_id
submission_data = submission_data[['series_id', 'step', 'event', 'score']]
submission_data['row_id'] = range(len(submission_data))

# Réordonner les colonnes
submission_data = submission_data[['row_id', 'series_id', 'step', 'event', 'score']]

# Sauvegarder le DataFrame dans un fichier CSV pour la soumission
submission_data.to_csv('submission.csv', index=False)

submission_data