# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oj4JI89qovkhIJye5O_fK66WilGXhoBy
"""

import numpy as np
import pandas as pd
import pyarrow
import pyarrow.parquet as pq
import fastparquet as fpq
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import random
import polars as pl

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Conv1D, Flatten
from keras.layers import MaxPooling1D, BatchNormalization
from scikeras.wrappers import KerasClassifier
#from keras.wrappers.scikit_learn import KerasClassifier

# Optimizers
import tensorflow as tf
import keras
from tensorflow.keras import optimizers

# Useful Sklearn library
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split
import sklearn.metrics


import polars as pl
train_series = (pl.scan_parquet('train_series.parquet')
                .with_columns(
                    (
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.year().alias("year")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.month().alias("month")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.day().alias("day")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.hour().alias("hour")),
                    )
                )
                .collect()
                .to_pandas()
               )

train_series

train_events = (pl.scan_csv('train_events.csv')
                .with_columns(
                    (
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.year().alias("year")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.month().alias("month")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.day().alias("day")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.hour().alias("hour")),
                    )
                )
                .collect()
                .to_pandas()
               )

train_events.head()

train_series = train_series.sort_values('timestamp').reset_index(drop=True)
train_series.head()

train_events = train_events.sort_values('timestamp').reset_index(drop=True)
train_events.head()

train_events[train_events['series_id']=="03d92c9f6f8a"]

# Identifiez les groupes qui contiennent au moins un NaN
to_delete = train_events.groupby('series_id').transform(lambda x: x.isna().any())

# Utilisez ce masque pour Ã©liminer ces groupes du DataFrame
train_events_tomerge = train_events.loc[~to_delete.any(axis=1)]

train_events_tomerge = train_events_tomerge.reset_index(drop=True)

series = train_events_tomerge['series_id'].unique().tolist()

#train_events_tomerge = train_events_tomerge[['timestamp', 'event']]
len(train_events_tomerge.groupby('series_id'))

series

train_series_merged=train_series[train_series['series_id'].isin(series)]

train_series_merged

train_series_merged = train_series_merged.merge(train_events_tomerge, how='left', on='timestamp')

train_series_merged.tail()

train_series_merged['event'] = train_series_merged.event.fillna(method='ffill')

train_series_merged['event'] = train_series_merged.event.fillna('wakeup')

train_series_merged



train_series_kaggle = pd.read_parquet('train_series_preprocessed.parquet')

train_series_kaggle[train_series_kaggle['series_id']=="fe90110788d2"]

train_series_merged.to_parquet('train_series_preprocessed_moi.parquet')