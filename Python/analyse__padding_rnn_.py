# -*- coding: utf-8 -*-
"""ANALYSE_ Padding&RNN_OK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/155PjIcqxAU-ops8xohV1FkuafZVoCA8o
"""

from google.colab import drive
drive.mount('/content/drive')

pip install fastparquet

pip install scikeras

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Conv1D, Flatten
from keras.layers import MaxPooling1D, BatchNormalization
from scikeras.wrappers import KerasClassifier
#from keras.wrappers.scikit_learn import KerasClassifier

# Optimizers
import tensorflow as tf
import keras
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Useful Sklearn library
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split
import sklearn.metrics
from keras.utils import plot_model

import numpy as np
import pandas as pd
import pyarrow
import pyarrow.parquet as pq
import fastparquet as fpq
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import random
import polars as pl
np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

"""# DONNEES TRAIN ET VALIDATION"""

# OUVERTURE DES DONNEES

data_train = pd.read_parquet("/content/drive/MyDrive/AIAO_Kaggle/Data/train_series_preprocessed_moi.parquet")

data_train

"""### NORMALISATION DES DONNEES

"""

# entre 0 et 1 avec minmaxscaler
scaler = MinMaxScaler()

# Fit and transform the 'anglez' and 'enmo' columns
data_train[['anglez', 'enmo']] = scaler.fit_transform(data_train[['anglez', 'enmo']])

# rendre Y (event) binary
data_train['event'] = np.where(data_train['event'] == 'wakeup', 1, 0)

data_train

"""### PADDING

"""

selected_data = data_train[data_train.index % 50 == 0]

# Séparation des IDs de séries en train et test
unique_series_ids = selected_data['series_id'].unique()
train_series_ids, test_series_ids = train_test_split(unique_series_ids, test_size=0.2, random_state=42)

# Filtrage des données en utilisant les IDs de séries
train_data_filtered = selected_data[selected_data['series_id'].isin(train_series_ids)]
test_data_filtered = selected_data[selected_data['series_id'].isin(test_series_ids)]

#len(selected_data)

line_counts = selected_data.groupby('series_id').count().max()
#line_counts

max_sequence_length = 12607
def pad_sequences(df):
    padded_sequences = []
    for series_id, group in df.groupby('series_id', sort=False):
        enmo_values = group['enmo'].values
        anglez_values = group['anglez'].values
        event_values = group['event'].values
        sequence = {'enmo': enmo_values, 'anglez': anglez_values, 'event': event_values}

        padded_enmo = tf.keras.utils.pad_sequences([sequence['enmo']], maxlen=max_sequence_length, padding="post", value=-1, dtype='float32')[0]
        padded_anglez = tf.keras.utils.pad_sequences([sequence['anglez']], maxlen=max_sequence_length, padding="post", value=-1, dtype='float32')[0]
        padded_event = tf.keras.utils.pad_sequences([sequence['event']], maxlen=max_sequence_length, padding="post", value=-1, dtype='float32')[0]
        padded_sequences.append({'enmo': padded_enmo, 'anglez': padded_anglez, 'event': padded_event})

    concatenated_enmo = np.concatenate([item['enmo'] for item in padded_sequences])
    concatenated_anglez = np.concatenate([item['anglez'] for item in padded_sequences])
    concatenated_event = np.concatenate([item['event'] for item in padded_sequences])
    padded_data = pd.DataFrame({'enmo': concatenated_enmo, 'anglez': concatenated_anglez, 'event': concatenated_event})

    return padded_data

train_padded_data = pad_sequences(train_data_filtered)
test_padded_data = pad_sequences(test_data_filtered)

"""### SEPARATION X ET Y + reshape

"""

# Séparation de X et Y
X_train_1 = train_padded_data[['anglez', 'enmo']].values
Y_train_1 = train_padded_data['event'].values

X_test_1= test_padded_data[['anglez', 'enmo']].values
Y_test_1 = test_padded_data['event'].values


X_test_1

# Reshape pour RNN
num_series_train = len(train_series_ids)
num_series_test = len(test_series_ids)

X_train = X_train_1.reshape(num_series_train, max_sequence_length, 2)
Y_train = Y_train_1.reshape(num_series_train, max_sequence_length, 1)

X_test = X_test_1.reshape(num_series_test, max_sequence_length, 2)
Y_test = Y_test_1.reshape(num_series_test, max_sequence_length, 1)

X_test.shape
Y_test.shape

"""# MEMES ETAPES POUR DONNEES TEST

"""

data_test = pd.read_parquet("/content/drive/MyDrive/Projet_Kaggle/PRE-FINAL_DATA/test_series.parquet")

data_test

"""### NORMALISATION ANGLEZ ET ENMO"""

# entre 0 et 1 avec minmaxscaler
scaler = MinMaxScaler()

# Fit and transform the 'anglez' and 'enmo' columns
data_test[['anglez', 'enmo']] = scaler.fit_transform(data_test[['anglez', 'enmo']])

data_test

"""### PADDING"""

# Initialize a list to store the padded sequences
padded_sequences_test = []

# Define the maximum sequence length (adjust this according to your data)
max_sequence_length = 564839

for series_id, group in data_test.groupby('series_id', sort=False):
    enmo_val = group['enmo'].values  # No need to convert to a list
    anglez_val = group['anglez'].values  # No need to convert to a list

    # Create a dictionary to hold the values
    seq = {'enmo': enmo_val, 'anglez': anglez_val}

    # Pad sequences using tf.keras.utils.pad_sequences
    pad_enmo = tf.keras.utils.pad_sequences([seq['enmo']], maxlen=max_sequence_length, padding="post", value=-1, dtype='float32')[0]
    pad_anglez = tf.keras.utils.pad_sequences([seq['anglez']], maxlen=max_sequence_length, padding="post", value=-1, dtype='float32')[0]

    # Append the padded sequences to the list
    padded_sequences_test.append({'enmo': pad_enmo, 'anglez': pad_anglez})

# Concatenate the sequences in the list
concat_enmo = np.concatenate([item['enmo'] for item in padded_sequences_test])
concat_anglez = np.concatenate([item['anglez'] for item in padded_sequences_test])

# Create a new DataFrame with the padded values
padded_data_test = pd.DataFrame({'enmo': concat_enmo, 'anglez': concat_anglez})

padded_data_test

"""### RESHAPE"""

# Extraire les valeurs du DataFrame
test_values = padded_data_test.values


# Appliquer reshape sur le tableau NumPy
X_test_p = test_values.reshape(3, 564839, 2)

print(X_test_p.shape)

"""# DEEP LEARNING RNN"""

import sklearn
import matplotlib.pyplot as plt
import os
import requests
from tensorflow.keras.optimizers import Adam
from keras.layers import Activation, SimpleRNN, Masking

pip install rnn

"""### SPLIT DES DONNEES EN TRAIN 80% ET VALIDATION 20%"""

#X_train, X_test, Y_train, Y_test  = train_test_split(X,Y,test_size=0.2, random_state=42)

X_test.shape

Y_train.shape

"""MODELE"""

def RNN_simple():
    model = Sequential()
    model.add(Masking(mask_value = -1))
    model.add(SimpleRNN(64, input_shape=(12607, 2), return_sequences =True))
    model.add(Dropout(0.2))
    model.add(SimpleRNN(128, return_sequences =True ))
    model.add(Dropout(0.2))
    model.add(SimpleRNN(64, return_sequences =True ))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    #model.add(Flatten())

	# compile model
    opt = Adam(learning_rate=0.0001)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return model

model_rnn = RNN_simple()

"""### FIT DU MODELE"""

history = model_rnn.fit(X_train, Y_train, validation_split = 0.2, epochs=60, batch_size= 256)

"""### VISUALISATION ACCURACY"""

print(history.history.keys())
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
# plt.savefig("images_cnn/Model_accuracy.png")
# plt.close()

"""### VISUALISATION LOSS"""

print(history.history.keys())
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#plt.savefig("images_cnn/Model_loss.png")
# plt.close()

"""### EVALUATION DU MODELE"""

evaluation_rnn = model_rnn.evaluate(X_test,Y_test)

print("Accuracy: %.2f" % (evaluation_rnn[1]))
print('Loss: %2f' % (evaluation_rnn[0]))

"""## AVEC DONNEES TEST (X_test_p)"""

y_pred_rnn = model_rnn.predict(X_test_p)
y_pred_rnn_flattened = y_pred_rnn.flatten()
y_pred_rnn_classes = [1 if prob >= 0.5 else 0 for prob in y_pred_rnn_flattened]

y_pred_rnn_classes

"""### Ajout des events predits dans dataframe"""

data_test["event_predictions"] = y_pred_rnn_classes[0:450]

data_test['event_predictions'] = data_test['event_predictions'].replace(0, "onset")
data_test['event_predictions'] = data_test['event_predictions'].replace(1, "wakeup")

data_test

#SAUVEGRADE

data_test.to_csv('RNN_predic_Kaggle_event.csv', index=False)

"""AVEC DONNEES DE VALIDATION"""

y_pred_rnn = model_rnn.predict(X_test)
y_pred_rnn_flattened = y_pred_rnn.flatten()
y_pred_rnn_classes = [1 if prob >= 0.5 else 0 for prob in y_pred_rnn_flattened]

len(y_pred_rnn_classes)
y_pred_rnn_classes

# Supposons que vous ayez les prédictions dans une liste ou un array nommé `y_pred`

if isinstance(X_test_1,np.ndarray) :
  X_test_2 = pd.DataFrame(X_test_1,columns=['anglez','enmo'])


X_test_2['y_pred']= y_pred_rnn_classes

X_test_2

# Convertir les prédictions en événements lisibles (par exemple, "wakeup" pour 1)
y_test_pred_events = np.where(y_pred_rnn_classes==1, 'wakeup', 'onset')  # Assumer que 1 correspond à 'wakeup'

# Convertir y_test_pred en une série pandas avec les mêmes indices que X_test
y_test_pred_series = pd.Series(y_test_pred_events, index=X_test_2.index)

# Extraire les lignes correspondantes de train_data en utilisant les indices de X_test
subset_train_data = data_train.loc[X_test_2.index]

# Ajouter les prédictions en tant que nouvelle colonne 'event'
subset_train_data['event_pred'] = y_test_pred_series
subset_train_data['event'] = subset_train_data['event'].apply(lambda x: 'wakeup' if x in [1, -1] else 'onset')

subset_train_data_sorted = subset_train_data.sort_values(by=['series_id', 'timestamp']).reset_index(drop=True)
subset_train_data_sorted

y_pred_series = pd.Series(y_pred_rnn_classes)
y_test_pred_events = y_pred_series.apply(lambda x: 'wakeup' if x == 1 else 'onset')

# Convertir y_test_pred en une série pandas avec les mêmes indices que X_test
y_test_pred_series = pd.Series(y_test_pred_events, index=X_test_2.index)

# Extraire les lignes correspondantes de train_data en utilisant les indices de X_test
subset_train_data = data_train.loc[X_test_2.index]

# Ajouter les prédictions en tant que nouvelle colonne 'event'
subset_train_data['event_pred'] = y_test_pred_series
subset_train_data['event'] = subset_train_data['event'].apply(lambda x: 'wakeup' if x in [1, -1] else 'onset')

subset_train_data_sorted = subset_train_data.sort_values(by=['series_id', 'timestamp']).reset_index(drop=True)
subset_train_data_sorted

y_test_pred_events



#subset_train_data_sorted[subset_train_data_sorted['series_id']=='31011ade7c0a']

subset_train_data_sorted.to_csv('data_to_analyze_RNN.csv', index=False)

"""##Analyse"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(data=subset_train_data_sorted, x='event_pred',palette={"wakeup": "orange", "onset": "green"})
plt.title('Nombre de wakeup et onset prédits')
plt.show()

sns.countplot(data=subset_train_data_sorted, x='event',palette={"wakeup": "orange", "onset": "green"})
plt.title('Nombre de wakeup et onset')
plt.show()

len(subset_train_data_sorted[subset_train_data_sorted['event_pred']=='onset'])

len(subset_train_data_sorted[subset_train_data_sorted['event_pred']==''])

sns.boxplot(data=subset_train_data_sorted, x='event', y='anglez')
plt.title('Distribution des valeurs d\'anglez pour chaque event')
plt.show()

sns.boxplot(data=subset_train_data_sorted, x='event', y='anglez')
plt.title('Distribution des valeurs d\'enmo pour chaque event')
plt.show()

#Matrice de confusion entre event et event_pred :

cm = confusion_matrix(subset_train_data_sorted['event'], subset_train_data_sorted['event_pred'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Prédiction')
plt.ylabel('Vérité')
plt.title('Matrice de confusion entre event et event_pred')
plt.show()

#Matrice de confusion entre event et event_pred :
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(subset_train_data_sorted['event'], subset_train_data_sorted['event_pred'])

# Normalisation de la matrice de confusion pour obtenir des pourcentages
cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
cm_percentage = cm_percentage * 100  # Convertir en pourcentages

# Affichage de la matrice de confusion avec les pourcentages
sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', cbar=False)
plt.xlabel('Prédiction')
plt.ylabel('Vérité')
plt.title('Matrice de confusion entre event et event_pred (%)')
plt.show()



#Histogramme du nombre de prédictions correctes par series_id :

subset_train_data_sorted['correct'] = subset_train_data_sorted['event'] == subset_train_data_sorted['event_pred']
correct_counts = subset_train_data_sorted.groupby('series_id')['correct'].sum()
correct_counts.plot(kind='bar', color='skyblue')
plt.title('Individus associés aux évenements les mieux prédits')
plt.xlabel('series_id')
plt.ylabel('Nombre de prédictions correctes')
plt.show()

most_common_series_id = subset_train_data_sorted['series_id'].value_counts().idxmax()
number_of_rows = subset_train_data_sorted['series_id'].value_counts().max()

most_common_series_id

import seaborn as sns
import matplotlib.pyplot as plt

# Filtrer les données pour cet individu
individual_data = subset_train_data_sorted[subset_train_data_sorted['series_id'] == '31011ade7c0a']

# Visualiser le nombre de onset et wakeup pour event_pred
plt.figure(figsize=(8,6))
sns.countplot(data=individual_data, x='event_pred', order=["onset", "wakeup"], palette=['blue', 'orange'])
plt.title('Nombre de prédictions onset et wakeup (event_pred) pour 31011ade7c0a')
plt.show()

# Visualiser le nombre de onset et wakeup pour event
plt.figure(figsize=(8,6))
sns.countplot(data=individual_data, x='event', order=["onset", "wakeup"], palette=['blue', 'orange'])
plt.title('Nombre réel d\'onset et de wakeup (event) pour 31011ade7c0a')
plt.show()