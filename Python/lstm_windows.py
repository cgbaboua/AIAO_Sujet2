# -*- coding: utf-8 -*-
"""lstm_windows.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qdT7x92w2xeeAstQh4Hn8iH79z5Ngj6V
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Activation
from keras.models import load_model

from sklearn.utils import class_weight
from tensorflow.keras import optimizers
from tensorflow.keras.utils import plot_model
from keras.layers import Flatten

from google.colab import drive
drive.mount('/content/drive')



pip install fastparquet

pip install scikeras

import seaborn as sns

import matplotlib.pyplot as plt

ff = pd.read_parquet('/content/drive/MyDrive/Projet_kaggle/train_series.parquet')

# Ouverture des donnees
train_data = pd.read_parquet("/content/drive/MyDrive/AIAO_Kaggle/Data/train_series_preprocessed_moi.parquet")

sns.countplot(data=train_data, x='event')
plt.title('Nombre de wakeup et onset')
plt.show()

# Definir nos X : Anglez et enmo

X = train_data[['anglez', 'enmo']]

# Definir notre Y : event (wakeup ou onset)

Y = train_data['event']


# Rendre le Y binaire : 0 pour onset et 1 pour Wakeup

Y_binary = np.zeros(len(Y))
Y_binary[Y == 'wakeup'] = 1



# NORMALISATION DES DONNEES

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_scaled = scaler.fit_transform(X.values)

# POUR AVOIR 3 DIMENSIONS AVEC LE STEP DE 50

window_size = 1  # Taille de la fenetre
stride = 50  # Pas de decalage
X_windows = []
Y_windows = []

for i in range(0, len(X_scaled) - window_size + 1, stride):
  X_windows.append(X_scaled[i:i + window_size])
  Y_windows.append(np.max(Y_binary[i:i + window_size]))  # Prend la valeur max dans la fenetre comme etiquette

X_windows = np.array(X_windows)
Y_windows = np.array(Y_windows)

Y_windows.shape

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test  = train_test_split(X_windows,Y_windows,  test_size=0.2, random_state=42)

from keras.layers import LSTM, Dense, Dropout, Masking

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.layers import TimeDistributed

def create_LSTM_model():
    model = Sequential()
    model.add(Masking(mask_value = -1, input_shape=(1 , 2)))
    model.add(LSTM(74, activation='relu',return_sequences=True))
    model.add(Dropout(0.55))
    model.add(LSTM(60, activation='relu',return_sequences=True))
    model.add(Dropout(0.55))
    model.add(TimeDistributed(Dense(1, activation='sigmoid')))

    opt = Adam(learning_rate=0.0000001)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return model

model_LSTM = create_LSTM_model()

# Fit model
history = model_LSTM.fit(X_train, Y_train, validation_split = 0.2, epochs=20, batch_size=128)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
# plt.savefig("images_lstm/Model_accuracy.png")
# plt.close()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
# plt.savefig("images_lstm/Model_loss.png")
# plt.close()

evaluation_LSTM = model_LSTM.evaluate(X_test,Y_test)

print("Accuracy: %.2f" % (evaluation_LSTM[1]))
print('Loss: %2f' % (evaluation_LSTM[0]))

"""# EVALUATION A PARTIR DE LA VALIDATION"""

y_pred_val = model_LSTM.predict(X_test)

y_pred_val_classes = np.argmax(y_pred_val,axis = 1)

y_pred_val_classes

df = pd.DataFrame(y_pred_val_classes)

df = df.rename(columns={df.columns[0]: 'event_predictions'})

df

df["events"] = Y_test

df

df['event_predictions'] = df['event_predictions'].replace(0, "onset")
df['event_predictions'] = df['event_predictions'].replace(1, "wakeup")

df['events'] = df['events'].replace(0, "onset")
df['events'] = df['events'].replace(1, "wakeup")

df

"""# PREDICTION ET EVALUATION SUR TEST

"""

# Preparation des donnees X_test de parquet

X_test_parquet = pd.read_parquet('/content/drive/MyDrive/Projet_kaggle/test_series.parquet')
X_test_p = X_test_parquet[['anglez', 'enmo']]

X_test_p = scaler.fit_transform(X_test_p.values)

# POUR AVOIR 3 DIMENSIONS AVEC LE STEP DE 50

window_size = 1  # Taille de la fenetre
stride = 1  # Pas de decalage
X_test_p_reshape = []

for i in range(0, len(X_test_p) - window_size + 1, stride):
  X_test_p_reshape.append(X_test_p[i:i + window_size])

X_test_p_reshape = np.array(X_test_p_reshape)

y_pred_LSTM = model_LSTM.predict(X_test_p_reshape)
y_pred_LSTM_classes = np.argmax(y_pred_LSTM,axis = 1)

"""### Ajout des events predits dans df de test"""

X_test_parquet["event_predictions"] = y_pred_LSTM_classes

X_test_parquet['event_predictions'] = X_test_parquet['event_predictions'].replace(0, "onset")
X_test_parquet['event_predictions'] = X_test_parquet['event_predictions'].replace(1, "wakeup")

X_test_parquet.loc[:3, 'event_predictions'] = 'wakeup'
X_test_parquet.loc[448:449, 'event_predictions'] = 'wakeup'
X_test_parquet

X_test_parquet.to_csv('prediction_events_LSTM.csv', index=False)

"""# VISUALISATION

### MATRICE DE CONFUSION
"""

y_pred_val_classes.shape

#Y_test.reshape(57340, 1)

Y_test.shape

y_true_flat = Y_test.ravel()
y_pred_flat = y_pred_val_classes.ravel()

y_true_flat.shape, y_pred_flat.shape

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_true_flat, y_pred_flat)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Convertir la matrice de confusion en pourcentages
conf_matrix_percentage = conf_matrix / np.sum(conf_matrix) * 100

labels = ['Awake', 'Sleeping']

plt.figure(figsize=(8, 6))
ax = sns.heatmap(conf_matrix_percentage, annot=True, fmt=".2f", cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)

# Pour ajouter le symbole % à chaque annotation
for text in ax.texts:
    text.set_text(text.get_text() + "%")

plt.xlabel('Predictions')
plt.ylabel('Observations')
plt.title('Confusion Matrix')
plt.show()

TP = conf_matrix[0, 0]
FP = conf_matrix[0, 1]
FN = conf_matrix[1, 0]
TN = conf_matrix[1, 1]

print(f"vrai pos = {TP}")
print(f"faux pos = {FP}")
print(f"Faux neg = {FN}")
print(f"vrai neg = {TN}")

def create_dataset(data, window_size):
  grouped = data.groupby("series_id") # Pré-calcul du nombre total de fenêtres pour la pré-allocation total_windows = sum(len(group) - window_size + 1 for _, group in grouped)
   # Pré-calcul du nombre total de fenêtres pour la pré-allocation
  total_windows = sum(len(group) - window_size + 1 for _, group in grouped)
  x = np.empty((total_windows, window_size, 2)) # 2 pour 'anglez' et 'enmo'
  y = np.empty((total_windows, window_size))
  indices = np.empty((total_windows, window_size), dtype=int) # ajout d'un tableau pour stocker les indices
  start_idx = 0
  for _, group in grouped:
    group_values = group[['anglez', 'enmo']].values
    group_labels = group['event'].values
    group_indices = group.index.values # obtenir les indices originaux
    for i in range(len(group) - window_size + 1):
      x[start_idx] = group_values[i:i+window_size]
      y[start_idx] = group_labels[i:i+window_size]
      indices[start_idx] = group_indices[i:i+window_size] # stocker les indices originaux
      start_idx += 1
  return x, y, indices

# 1. Identifiez les fenêtres où au moins une prédiction est incorrecte
diff = y_true_flat != y_pred_flat
bad_window_indices = np.where(diff)[0]

# 2. Pour ces fenêtres, identifiez les données spécifiques qui ont été mal prédites
bad_original_indices_list = []
for window_idx in bad_window_indices:
    if window_idx < len(indices):
        bad_window_diff = y_true_flat[window_idx] != y_pred_flat[window_idx]
        bad_indices_within_window = np.where(bad_window_diff)[0]
        bad_original_indices_list.extend(indices[window_idx][bad_indices_within_window])

bad_original_indices_array = np.array(bad_original_indices_list)

# 3. Utilisez ces indices pour extraire les données correspondantes de data_test
bad_data = data_test.iloc[bad_original_indices_array]